# 从零手搓大语言模型（一）：用三国演义训练你的第一个语言模型

> 这是「从零手搓 LLM」系列的第一篇。我们从最简单的 Bigram（二元）语言模型开始，用《三国演义》全文作为训练数据，在 CPU 上跑通从训练到生成的完整流程。代码不到 200 行，不需要 GPU。

## 什么是语言模型？

语言模型的本质就一句话：**给定前面的文字，预测下一个字是什么。**

当你在手机输入法里打了"今天天气"，它帮你联想出"真好"——这就是一个语言模型在工作。ChatGPT 也是同样的原理，只不过它用了更大的模型、更多的数据、更复杂的结构。

今天我们搭建的是最最最简单的版本：**Bigram 模型**——只看当前这一个字，就预测下一个字。

## 为什么从 Bigram 开始？

你可能会问：只看一个字能有什么用？生成的文字肯定不像话啊？

没错。Bigram 模型的生成质量很差，但这恰恰是它的教学价值：

1. **它足够简单**——整个模型只有一张查找表，没有任何花哨的结构，让你把注意力放在理解流程上
2. **它跑通了完整的 pipeline**——分词、训练、损失计算、反向传播、文本生成，一个不少
3. **它是后续改进的基线**——后面我们加注意力机制、加 Transformer 层，每一步都能对比 Bigram 看到明确的提升

## 先看全貌：一个 LLM 由什么组成？

在动手写代码之前，我们先从最高的抽象层理解大语言模型。不管是 GPT、LLaMA 还是 DeepSeek，所有 LLM 都可以拆成以下几个核心积木：

```
"却说曹操"  ──→  [ Tokenization ]  ──→  [2187, 541, 1038, 2893]
   原始文本        分词：文字变数字            Token 序列

                        ↓

[2187, 541, 1038, 2893]  ──→  [ Embedding ]  ──→  [[0.12, -0.5, ...], ...]
      Token 序列               嵌入：数字变向量          向量序列

                        ↓

[[0.12, -0.5, ...], ...]  ──→  [ Transformer ]  ──→  [[0.87, 0.23, ...], ...]
      向量序列                   模型核心：理解上下文          新的向量序列
                          ┌─────────────────────┐
                          │  Self-Attention      │  ← 让每个字"看到"其他字
                          │  Feed-Forward (FFN)  │  ← 对每个位置做非线性变换
                          │  Layer Norm          │  ← 稳定训练
                          │  × N 层堆叠          │  ← 层数越多，理解越深
                          └─────────────────────┘

                        ↓

[[0.87, 0.23, ...], ...]  ──→  [ Linear + Softmax ]  ──→  "引" (概率最高)
      新的向量序列               输出层：向量变概率              预测下一个字
```

### 六个核心概念

| 概念 | 一句话解释 | 本篇涉及？ |
|------|-----------|:----------:|
| **Tokenization** | 把文字切成数字序列，是所有 NLP 的第一步 | ✅ 字符级分词 |
| **Embedding** | 把离散的 token 编号映射为连续的向量，让模型能做数学运算 | ✅ nn.Embedding |
| **Self-Attention** | 让序列中的每个位置能"看到"并"关注"其他位置，是 Transformer 的灵魂 | ❌ 下一步 |
| **Feed-Forward Network (FFN)** | 对每个位置独立做非线性变换，增加模型的表达能力 | ❌ 下一步 |
| **Loss & Backpropagation** | 衡量预测有多差（Loss），然后反向计算每个参数该怎么调（梯度） | ✅ cross_entropy + backward |
| **Autoregressive Generation** | 逐字生成：预测一个字 → 拼到输入后面 → 再预测下一个字，循环往复 | ✅ generate() |

今天的 Bigram 模型跳过了 Attention 和 FFN（所以它不是 Transformer），但**其余四个概念全部用到了**。这意味着后续升级时，我们只需要在 Embedding 和输出层之间"插入" Transformer 结构，其他部分可以原封不动地复用。

---

## 全景路线图：从 Bigram 到 GPT

理解了核心积木后，我们来看完整的训练流程：

```
┌─────────────────────────────────────────────────────────────────┐
│                    LLM 训练全景路线图                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ① 数据准备                                                     │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐                  │
│  │ 收集原始  │ →  │ 清洗/去重 │ →  │ 分词编码  │                  │
│  │ 文本语料  │    │ 预处理    │    │Tokenizer │                  │
│  └──────────┘    └──────────┘    └──────────┘                  │
│                                                                 │
│  ② 模型架构                                                     │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐                  │
│  │ Embedding │ →  │ Attention │ →  │Transformer│                 │
│  │ 词嵌入层  │    │ 注意力机制│    │ 多层堆叠  │                  │
│  └──────────┘    └──────────┘    └──────────┘                  │
│                                                                 │
│  ③ 训练过程                                                     │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐                  │
│  │ 前向传播  │ →  │ 计算损失  │ →  │ 反向传播  │                  │
│  │ + 预测    │    │ Loss     │    │ + 更新参数│                  │
│  └──────────┘    └──────────┘    └──────────┘                  │
│                                                                 │
│  ④ 生成推理                                                     │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐                  │
│  │ 输入提示  │ →  │ 自回归采样│ →  │ 输出文本  │                  │
│  │ Prompt    │    │ 逐字生成  │    │ Response │                  │
│  └──────────┘    └──────────┘    └──────────┘                  │
│                                                                 │
│  ⑤ 对齐优化（进阶）                                              │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐                  │
│  │ SFT      │ →  │ RLHF/DPO │ →  │ 评估测试  │                  │
│  │ 指令微调  │    │ 人类偏好  │    │Benchmark │                  │
│  └──────────┘    └──────────┘    └──────────┘                  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 我们的学习计划

| 步骤 | 内容 | 对应路线图 | 状态 |
|------|------|-----------|------|
| **第 1 步 👈 本篇** | **Bigram 模型** | ① 字符级分词 + ② Embedding + ③ 完整训练 + ④ 自回归生成 | **已完成** |
| 第 2 步 | Self-Attention | ② 加入注意力机制，模型能"看到"前文多个字符 | 待做 |
| 第 3 步 | Multi-Head Attention + FFN | ② 多头注意力 + 前馈网络，构成完整 Transformer Block | 待做 |
| 第 4 步 | Mini-GPT | ② 多层 Transformer 堆叠，真正的"小型 GPT" | 待做 |
| 第 5 步 | BPE 分词器 | ① 从字符级升级到子词级分词 | 待做 |
| 第 6 步 | 在自定义数据上微调 | ⑤ 让模型学会特定领域的表达 | 待做 |

可以看到，虽然今天的 Bigram 模型极其简单，但它**已经走通了 ①②③④ 四个大阶段**。后续每一步都是在这个骨架上升级某个模块，而不是推倒重来。这就是为什么我们从 Bigram 开始——它是麻雀虽小、五脏俱全的最小完整实现。

---

## 项目结构

```
myllm/
├── data/input.txt      # 训练数据：《三国演义》全文（~1.8MB）
├── model.py            # 模型定义
├── train.py            # 训练脚本
├── generate.py         # 文本生成脚本
└── bigram_model.pt     # 训练好的模型文件
```

---

## 第一步：字符级分词器

在喂给模型之前，我们要先把文字变成数字。最简单的方式就是**字符级分词**：每个字符（包括汉字、标点、空格）分配一个唯一的整数编号。

```python
chars = sorted(set(text))       # 从文本中提取所有不重复的字符
vocab_size = len(chars)          # 词表大小

stoi = {ch: i for i, ch in enumerate(chars)}  # 字符 → 数字
itos = {i: ch for i, ch in enumerate(chars)}  # 数字 → 字符

encode = lambda s: [stoi[c] for c in s]       # "曹操" → [1234, 2345]
decode = lambda l: "".join([itos[i] for i in l])  # [1234, 2345] → "曹操"
```

《三国演义》全文包含 **4,742 个不同字符**，所以我们的词表大小 `vocab_size = 4742`。

### 为什么用字符级而不是词级？

- **零依赖**：不需要任何分词库，几行代码就搞定
- **概念清晰**：一个字符就是一个 token，没有歧义
- **适合入门**：先理解原理，后面再引入 BPE 等更高级的分词方法

代价是词表相对较大（中文汉字多），但对于学习目的完全可以接受。

---

## 第二步：模型——一张查找表

Bigram 模型的核心思想极其简单：

> 对于每个字符 A，我要学会"A 后面最可能出现哪些字符"。

这本质上就是一张 **vocab_size × vocab_size** 的概率表。第 i 行存的是"字符 i 出现后，下一个字符的概率分布"。

在 PyTorch 里，我们用 `nn.Embedding` 来实现这张表：

```python
class BigramLanguageModel(nn.Module):
    def __init__(self, vocab_size: int):
        super().__init__()
        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)
```

### nn.Embedding 是什么？

你可以把 `nn.Embedding` 想成一本**字典**：你给它一个字的编号，它返回对应的一行数据。

在我们的场景中，这本"字典"有 4742 页（每个字一页），每页写着 4742 个数字（对下一个字的打分）。所以整张表就是 4742 × 4742 = **22,486,564**（约 2250 万）个数字，这就是模型需要学习的全部内容。

它和普通的 Excel 表格的区别是：`nn.Embedding` 能**参与训练**。在训练过程中，PyTorch 会自动帮我们调整表里的数字，让预测越来越准。如果只用普通数组，我们就得自己手写一堆数学公式来更新数值，麻烦得多。

---

## 第三步：前向传播与损失函数

```python
def forward(self, idx, targets=None):
    logits = self.token_embedding_table(idx)     # (B, T, C)

    loss = None
    if targets is not None:
        B, T, C = logits.shape
        logits_flat = logits.view(B * T, C)      # (B*T, C)
        targets_flat = targets.view(B * T)        # (B*T,)
        loss = F.cross_entropy(logits_flat, targets_flat)

    return logits, loss
```

### 形状解释

你可以把数据想象成一个**立体的表格**：

- **B** (Batch) = 一批有多少条样本，好比全班 64 个学生
- **T** (Time) = 每条样本的长度，好比每个学生做了 8 道题
- **C** (Classes) = 词表大小（4742），好比每道题有 4742 个选项

输入 `idx` 形状是 `(B, T)` —— 64 个学生，每人一张 8 道题的试卷。经过 Embedding 查表后得到 `(B, T, C)` —— 每道题都附上了 4742 个选项的打分，就像给每个选项标了"我觉得这个选项有多靠谱"的分数。

### 为什么用 cross_entropy（交叉熵）？

打个比方：模型预测"曹"后面是什么字，它对 4742 个候选字各打了一个分。正确答案是"操"。

**cross_entropy 做的事就是给模型的回答打分：**
- 模型说"操"有 60% 的概率 → loss 比较小（猜得不错）
- 模型说"操"只有 1% 的概率 → loss 非常大（猜得很差）

所以 loss 就像考试扣分——猜得越离谱，扣分越狠。模型通过不断减少 loss 来学会给正确答案更高的概率。

本质上，我们在做的是一个 **4742 选 1 的选择题**：给你一个字，从 4742 个候选字里选出正确的下一个字。cross_entropy 就是专门给这种选择题打分的工具。

### 为什么需要 view/reshape？

这是一个纯粹的"格式要求"问题。

PyTorch 的 `F.cross_entropy` 规定：输入必须是一张二维表格 `(样本数, 类别数)`。但我们的数据是三维的 `(批次, 序列长度, 类别数)`——因为一批有 64 条样本，每条有 8 个位置，每个位置对应 4742 个候选字。

解决办法很简单：把 `64 × 8 = 512` 个位置**展平**成 512 个独立的样本，变成 `(512, 4742)` 的二维表格。就像把一本 64 页、每页 8 行的练习簿拆散，摊成 512 行的一长条。

---

## 第四步：训练超参数详解

```python
BATCH_SIZE = 64
BLOCK_SIZE = 8
MAX_STEPS = 10000
EVAL_INTERVAL = 1000
EVAL_ITERS = 200
LEARNING_RATE = 1e-2
```

逐个解释，你会发现每个数字背后都有道理：

### BATCH_SIZE = 64 —— 每次做多少道题

好比一个学生每次做多少道练习题：
- **1 道**（太少）：反馈太少，今天做的这一道碰巧简单，就以为自己全会了——学习方向容易跑偏
- **1024 道**（太多）：做不完（CPU 吃不消），而且做了一千道才改一次错，效率太低
- **64 道**（刚好）：每次做 64 道，既能看出规律，又不会太慢

### BLOCK_SIZE = 8 —— 每道题有几个字

每条训练样本截取连续 8 个字。对于 Bigram 模型来说，这个参数其实"不太重要"——因为它只看前 1 个字，不管上下文有多长。设为 8 是因为：
- 一条样本提供 8 个练习机会（"却→说"、"说→曹"、"曹→操"……连续 8 个"猜下一个字"的练习）
- 后面加了注意力机制后，这个值决定了模型能"看到"多远的上下文，到时候会调大

### MAX_STEPS = 10000 —— 总共练习多少轮

中文有 4742 个不同字符（远比英文 26 个字母复杂得多）。要学会这么多字之间的搭配关系，需要更多的练习次数。就像学中文比学英文要认更多字，自然需要更多时间。从训练日志可以看到 loss 在 10000 步时仍在缓慢下降，说明这个数字是合理的。

### EVAL_INTERVAL = 1000 —— 多久考一次试

就像考试不能每做一道题就对答案（太频繁，浪费时间），但也不能做完全部才看分数（发现错了已经太晚）。每 1000 轮看一次，10000 轮里看 10 次，刚好能看到进步趋势。

### EVAL_ITERS = 200 —— 考试出多少道题

每次"考试"随机抽 200 组题，取平均分。为什么不只出 1 组题？因为 1 组题算出来的分数波动太大——可能碰巧全是简单题，也可能碰巧全是难题。平均 200 组题的分数，才能反映真实水平。

### LEARNING_RATE = 1e-2（0.01）—— 每次纠正的力度

好比写书法时老师帮你扶手：
- **力度太大**（0.1）：矫枉过正，越写越歪
- **力度太小**（0.0001）：半天没变化，进步太慢
- **0.01**：对我们这个简单模型来说刚好

为什么不用更常见的 0.001？因为我们的模型结构极其简单（就一张查找表），词表又大（4742 个字），需要更大的步子才能在有限步数内学到位。后面模型变复杂后（Transformer），就要用更小的学习率（0.001 或 0.0003），不然会"手抖"。

---

## 第五步：数据加载与 batch 生成

### 训练/验证集划分

```python
n = int(0.9 * len(data))
train_data = data[:n]     # 前 90%
val_data = data[n:]        # 后 10%
```

**为什么要分验证集？** 想象一个学生只做课本上的原题——他可能把答案都背下来了（过拟合），但遇到新题就不会了。验证集就是"课本里没有的新题"，只有新题也能考好，才说明真的学会了。

从我们的训练日志可以验证这一点：

```
step     0 | train loss 8.9392 | val loss 8.9385   # 开始时两者接近（都是瞎蒙）
step  9999 | train loss 3.9272 | val loss 5.5611   # 训练集分数好，验证集差不少
```

训练集 loss 比验证集低不少——说明模型有点"背书"了（对教材很熟，但遇到新文本就差一些）。这对 Bigram 模型是正常的——2250 万个参数对一个只靠查表的模型来说"记忆力"太好了，很容易记住训练数据的特征。

### 随机 batch 采样

```python
def get_batch(split):
    d = train_data if split == "train" else val_data
    ix = torch.randint(len(d) - BLOCK_SIZE, (BATCH_SIZE,))
    x = torch.stack([d[i : i + BLOCK_SIZE] for i in ix])
    y = torch.stack([d[i + 1 : i + BLOCK_SIZE + 1] for i in ix])
    return x.to(DEVICE), y.to(DEVICE)
```

**工作方式**就像从一本书里随机翻开一页，抄下连续 8 个字：

1. 从三国演义里随机选 64 个位置（"翻开 64 页"）
2. 从每个位置截取连续 8 个字作为题目 `x`
3. 答案 `y` 就是题目往后挪一位——每个字的"正确的下一个字"

举个例子：如果翻到的一段是"却说曹操引兵追赶关"：
- 题目 x = `却说曹操引兵追赶`
- 答案 y = `说曹操引兵追赶关`

这样一条样本就提供了 8 个练习：却→说、说→曹、曹→操、操→引……

### 为什么随机翻页，而不是从头到尾顺着读？

- 每次练习看到的片段都不一样，不容易"背书"（减少过拟合）
- 不用操心"上次读到哪里了"，代码更简单
- 对 Bigram 模型来说，"曹后面跟操"这个规律不管出现在书的第几页都是一样的，不需要按顺序学

---

## 第六步：优化器——AdamW

```python
optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
```

### 为什么选 AdamW 而不是 SGD？

想象你在雾里爬山（找 loss 最低的那个谷底）：

**SGD（随机梯度下降）** 是最原始的爬法：每走一步就看看脚下的坡度，往下坡方向走固定的步长。问题是：
- 所有方向都迈一样大的步子，但有的方向坡陡（需要小步走），有的方向坡缓（需要大步走）
- 雾里什么都看不清，一阵风（噪声）就可能把你吹偏

**Adam** 聪明在两点：
1. **有"记忆"**：它会记住之前几步走的方向，有惯性。就像骑自行车，不会因为路上一个小石子就突然转向
2. **自动调步幅**：给每个参数分别调节步子大小。有的参数需要大步走，有的需要小步挪，Adam 自动搞定

**AdamW** 是 Adam 的改进版，在现代深度学习中几乎是默认选择——GPT 系列模型的训练也用它。

### optimizer.zero_grad(set_to_none=True)

每次练完一组题、对完答案之后，要把上次的"纠错记录"清空，不然新旧记录会混在一起，越来越乱。`set_to_none=True` 比默认方式更快——直接把旧记录丢掉，而不是逐个归零。

---

## 第七步：训练循环

```python
for step in range(MAX_STEPS):
    # 1. 每隔 1000 轮考一次试，看看成绩怎么样
    if step % EVAL_INTERVAL == 0 or step == MAX_STEPS - 1:
        losses = estimate_loss(model)
        print(...)

    # 2. 随机从书里抽一批练习题
    xb, yb = get_batch("train")

    # 3. 让模型做题，算出"猜错了多少"（loss）
    _, loss = model(xb, yb)

    # 4. 把上一轮的纠错记录清空
    optimizer.zero_grad(set_to_none=True)

    # 5. 对答案——算出"哪里错了、该往哪个方向改"（反向传播）
    loss.backward()

    # 6. 按照纠错方向，微调模型参数
    optimizer.step()
```

这就是所有深度学习训练的核心循环——不管是我们这个最简单的 Bigram 模型，还是几百亿参数的 GPT，训练过程都是这六步，只是模型本身更复杂而已。

### estimate_loss 中的 @torch.no_grad()

```python
@torch.no_grad()
def estimate_loss(model):
    model.eval()
    ...
    model.train()
```

这三行代码做的事情很好理解：

- `@torch.no_grad()`：考试的时候不需要"记笔记"（计算梯度），关掉可以省内存、跑更快
- `model.eval()`：切到"考试模式"。考试和做作业的规则不一样——有些训练专用的功能（比如后面会学到的 Dropout）在考试时要关掉，不然成绩不准
- `model.train()`：考完了，切回"做作业模式"继续训练

---

## 第八步：文本生成

```python
@torch.no_grad()
def generate(self, idx, max_new_tokens):
    for _ in range(max_new_tokens):
        logits, _ = self(idx)
        logits = logits[:, -1, :]           # 只取最后一个位置的预测
        probs = F.softmax(logits, dim=-1)   # logits → 概率分布
        idx_next = torch.multinomial(probs, num_samples=1)  # 按概率采样
        idx = torch.cat([idx, idx_next], dim=1)             # 拼接到序列末尾
    return idx
```

### 关键步骤解释

**F.softmax —— 把"打分"变成"概率"**

模型给 4742 个候选字打的分（logits）可能是 `[3.2, 1.0, -0.5, ...]` 这样的任意数字。softmax 的作用就是把这些分数变成"概率"：所有值都在 0~1 之间，加起来等于 1。比如 `[3.2, 1.0, -0.5]` → `[0.78, 0.09, 0.02, ...]`。

这就像考试后把原始分数换算成百分比——让你一眼就能看出每个选项有多大可能性。

**torch.multinomial —— 按概率抽奖**

有了概率之后，就像一个转盘抽奖：如果"操"占 30%、"军"占 20%、"兵"占 10%……转一次转盘，落到谁就选谁。概率大的区域面积大，被选中的机会自然更大。

### 为什么用"抽奖"而不是直接选概率最大的？

如果每次都选概率最高的（贪心解码），生成的文本会变成"车轱辘话"——反复说同样的内容。随机抽奖引入了多样性，就像人写文章也不会每次都用同样的措辞。

后续我们还会介绍 **temperature**（温度）参数来控制"转盘"的随机程度——温度高，选择更随机；温度低，越倾向于选概率最高的。

---

## 训练结果

```
Vocabulary size: 4742 unique characters
Model parameters: 22,486,564
Device: cpu

step     0 | train loss 8.9392 | val loss 8.9385
step  1000 | train loss 5.2032 | val loss 5.8429
step  5000 | train loss 4.0640 | val loss 5.4034
step  9999 | train loss 3.9272 | val loss 5.5611
```

### 初始 loss 为什么是 8.94？

想象一下：如果你完全不懂中文，随机从 4742 个字里猜下一个字，你猜对的概率是 1/4742。这种"纯蒙"对应的 loss 大约是 **8.46**。

实际初始 loss 8.94 比这还差一点，为什么？因为模型刚开始时，表里的数字是随机填的（PyTorch 用正态分布初始化），它的预测甚至不如"均匀地蒙"——有些字被给了不合理的高分，有些被打了不合理的低分。

不过没关系，训练过程就是不断修正这些分数，让它从"胡蒙"变成"有依据地猜"。

### 生成样本

输入"却说曹操"，生成的文本：

> 却说曹操贼未知苏顒缃毫无言，吏，都督建庙。"玄德曰："江人也。"盖宫为将翻。城，去。岂与德便连日内书遗图敌……

乍一看是乱码，但仔细看有些有趣的规律：
- 出现了三国人物名（曹操、玄德）——模型学到了"曹"后面大概率跟"操"
- 有"曰"字和引号——学到了古文对话的基本格式
- 但完全不通顺——这很正常，因为 Bigram 只看前面 1 个字就猜下一个字，就像一个人只记得"曹后面跟操"，但完全不理解"曹操"是一个人名

---

## 模型保存与加载

```python
# 保存
torch.save({
    "model_state_dict": model.state_dict(),
    "vocab_size": vocab_size,
    "stoi": stoi,
    "itos": itos,
}, "bigram_model.pt")
```

### 为什么只保存参数，不保存整个模型？

打个比方：你练了一手好字，要把成果保存下来。

- **保存整个模型** = 把你这个人连同笔墨纸砚一起装箱。如果以后换了桌子，可能箱子打不开
- **保存参数（state_dict）** = 只保存你练出来的"肌肉记忆"。以后不管换什么桌子，只要你还是你，功夫还在

所以 PyTorch 推荐只保存参数数值，不和代码结构绑死，更灵活。

我们额外保存了 `vocab_size`、`stoi`、`itos`（字→数字的对照表），这样 `generate.py` 加载时不需要再读三国演义原文就能把数字翻译回文字。

---

## 总结

恭喜你走完了第一步！虽然生成的文字还是一团糟，但我们已经跑通了一个完整的语言模型：

| 你学到了什么 | 一句话回顾 |
|-------------|-----------|
| **字符级分词** | 给每个字编个号，电脑只认数字 |
| **nn.Embedding** | 一本可以"自动学习"的字典，输入一个字的编号，返回对下一个字的打分 |
| **cross_entropy** | 给模型的回答打分——猜得越离谱，扣分越狠 |
| **AdamW** | 聪明的"纠错老师"，记得住之前的方向，还能自动调节力度 |
| **训练循环** | 做题 → 对答案 → 纠错 → 再做题，反复循环 |
| **自回归生成** | 写一个字 → 按概率抽奖选下一个字 → 拼上去 → 继续写 |

Bigram 模型虽然简陋，但它是理解语言模型的最佳起点——所有大模型的训练都是这个骨架，只是模型本身更复杂。下一篇，我们会给模型加上**自注意力机制（Self-Attention）**，让它不再只看前面 1 个字，而是能"看到"更多的上下文。到时候生成质量会有质的飞跃，敬请期待！

---

*代码仓库：[GitHub 链接]*
*使用的技术栈：Python 3.10 + PyTorch + uv*
*训练数据：《三国演义》全文（~60 万字符）*
*训练设备：CPU（无需 GPU）*
