# 文章撰写指南

> 本文件是所有文章共享的背景知识和撰写规范。写新文章时加载此文件，引用其中的概念框架，避免在每篇文章中重复解释。

## 系列定位

「从零手搓 LLM」系列，面向有一定编程基础但没有深度学习经验的读者，用《三国演义》作为训练数据，逐步搭建一个完整的语言模型。

## LLM 核心架构

所有 LLM（GPT、LLaMA、DeepSeek 等）都可以拆成以下数据流：

```
"却说曹操"  ──→  [ Tokenization ]  ──→  [2187, 541, 1038, 2893]
   原始文本        分词：文字变数字            Token 序列

                        ↓

[2187, 541, 1038, 2893]  ──→  [ Embedding ]  ──→  [[0.12, -0.5, ...], ...]
      Token 序列               嵌入：数字变向量          向量序列

                        ↓

[[0.12, -0.5, ...], ...]  ──→  [ Transformer ]  ──→  [[0.87, 0.23, ...], ...]
      向量序列                   模型核心：理解上下文          新的向量序列
                          ┌─────────────────────┐
                          │  Self-Attention      │  ← 让每个字"看到"其他字
                          │  Feed-Forward (FFN)  │  ← 对每个位置做非线性变换
                          │  Layer Norm          │  ← 稳定训练
                          │  × N 层堆叠          │  ← 层数越多，理解越深
                          └─────────────────────┘

                        ↓

[[0.87, 0.23, ...], ...]  ──→  [ Linear + Softmax ]  ──→  "引" (概率最高)
      新的向量序列               输出层：向量变概率              预测下一个字
```

## 六个核心概念

| 概念 | 一句话解释 |
|------|-----------|
| **Tokenization** | 把文字切成数字序列，是所有 NLP 的第一步 |
| **Embedding** | 把离散的 token 编号映射为连续的向量，让模型能做数学运算 |
| **Self-Attention** | 让序列中的每个位置能"看到"并"关注"其他位置，是 Transformer 的灵魂 |
| **Feed-Forward Network (FFN)** | 对每个位置独立做非线性变换，增加模型的表达能力 |
| **Loss & Backpropagation** | 衡量预测有多差（Loss），然后反向计算每个参数该怎么调（梯度） |
| **Autoregressive Generation** | 逐字生成：预测一个字 → 拼到输入后面 → 再预测下一个字，循环往复 |

## LLM 训练全景路线图

```
┌─────────────────────────────────────────────────────────────────┐
│                    LLM 训练全景路线图                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ① 数据准备                                                     │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐                  │
│  │ 收集原始  │ →  │ 清洗/去重 │ →  │ 分词编码  │                  │
│  │ 文本语料  │    │ 预处理    │    │Tokenizer │                  │
│  └──────────┘    └──────────┘    └──────────┘                  │
│                                                                 │
│  ② 模型架构                                                     │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐                  │
│  │ Embedding │ →  │ Attention │ →  │Transformer│                 │
│  │ 词嵌入层  │    │ 注意力机制│    │ 多层堆叠  │                  │
│  └──────────┘    └──────────┘    └──────────┘                  │
│                                                                 │
│  ③ 训练过程                                                     │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐                  │
│  │ 前向传播  │ →  │ 计算损失  │ →  │ 反向传播  │                  │
│  │ + 预测    │    │ Loss     │    │ + 更新参数│                  │
│  └──────────┘    └──────────┘    └──────────┘                  │
│                                                                 │
│  ④ 生成推理                                                     │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐                  │
│  │ 输入提示  │ →  │ 自回归采样│ →  │ 输出文本  │                  │
│  │ Prompt    │    │ 逐字生成  │    │ Response │                  │
│  └──────────┘    └──────────┘    └──────────┘                  │
│                                                                 │
│  ⑤ 对齐优化（进阶）                                              │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐                  │
│  │ SFT      │ →  │ RLHF/DPO │ →  │ 评估测试  │                  │
│  │ 指令微调  │    │ 人类偏好  │    │Benchmark │                  │
│  └──────────┘    └──────────┘    └──────────┘                  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## 学习计划

| 步骤 | 内容 | 对应路线图 | 涉及的核心概念 |
|------|------|-----------|---------------|
| 第 1 步 | Bigram 模型 | ①②③④ | Tokenization, Embedding, Loss & Backprop, Generation |
| 第 2 步 | Self-Attention | ② | Self-Attention |
| 第 3 步 | Multi-Head Attention + FFN | ② | Self-Attention, FFN |
| 第 4 步 | Mini-GPT | ②③ | 全部六个概念 |
| 第 5 步 | BPE 分词器 | ① | Tokenization |
| 第 6 步 | 自定义数据微调 | ⑤ | Loss & Backprop |

## 撰写规范

- **核心概念引用规则**：LLM 架构、六个核心概念、全景路线图、学习计划这些内容已在第 1 篇（`01-baby-step-bigram-model.md`）中完整呈现。后续文章**不再重复**，只需在开头写一句"关于 LLM 的整体架构和核心概念，请参阅本系列第一篇"并附上链接。
- **每篇文章开头**：用一段引言说明本篇在学习计划中的位置，标注本篇新增了哪些核心概念（如"本篇我们引入 Self-Attention"）
- **新概念首次出现时**：在该篇文章中详细解释；后续文章引用时只需一句话带过，指向首次出现的文章编号
- **代码讲解**：每段代码后面解释"为什么用这个 API"和"为什么选这个超参数"
- **训练结果**：展示 loss 曲线和生成样本，与上一步做对比
- **文末总结**：用表格列出本篇学到的组件，预告下一篇内容
