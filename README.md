# myllm â€” ä»é›¶æ‰‹æ“å¤§è¯­è¨€æ¨¡å‹

ä»æœ€ç®€å•çš„ Bigram æ¨¡å‹å¼€å§‹ï¼Œé€æ­¥æ„å»ºä¸€ä¸ªå®Œæ•´çš„ Transformer è¯­è¨€æ¨¡å‹ã€‚ç”¨ã€Šä¸‰å›½æ¼”ä¹‰ã€‹ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œæ¯ä¸€æ­¥éƒ½é…æœ‰è¯¦ç»†çš„ä¸­æ–‡æ•™ç¨‹æ–‡ç« ã€‚

## å­¦ä¹ è·¯çº¿

| æ­¥éª¤ | å†…å®¹ | çŠ¶æ€ |
|------|------|:----:|
| ç¬¬ 1 æ­¥ | [Bigram æ¨¡å‹](articles/01-baby-step-bigram-model.md) â€” å­—ç¬¦çº§åˆ†è¯ + Embedding + è®­ç»ƒå¾ªç¯ + æ–‡æœ¬ç”Ÿæˆ | âœ… |
| ç¬¬ 2 æ­¥ | Self-Attention â€” è®©æ¨¡å‹èƒ½"çœ‹åˆ°"å‰æ–‡å¤šä¸ªå­—ç¬¦ | ğŸ”œ |
| ç¬¬ 3 æ­¥ | Multi-Head Attention + FFN â€” æ„æˆå®Œæ•´ Transformer Block | ğŸ”œ |
| ç¬¬ 4 æ­¥ | Mini-GPT â€” å¤šå±‚ Transformer å †å  | ğŸ”œ |
| ç¬¬ 5 æ­¥ | BPE åˆ†è¯å™¨ â€” ä»å­—ç¬¦çº§å‡çº§åˆ°å­è¯çº§ | ğŸ”œ |
| ç¬¬ 6 æ­¥ | è‡ªå®šä¹‰æ•°æ®å¾®è°ƒ | ğŸ”œ |

## å¿«é€Ÿå¼€å§‹

```bash
# å®‰è£…ä¾èµ–
uv sync

# ä¸‹è½½è®­ç»ƒæ•°æ®ï¼ˆä¸‰å›½æ¼”ä¹‰ï¼‰
mkdir -p data
curl -o data/input.txt https://raw.githubusercontent.com/tennessine/corpus/master/%E4%B8%89%E5%9B%BD%E6%BC%94%E4%B9%89.txt

# è®­ç»ƒæ¨¡å‹
uv run python train.py

# ç”Ÿæˆæ–‡æœ¬
uv run python generate.py
uv run python generate.py --prompt "å´è¯´æ›¹æ“" --length 200
```

## æŠ€æœ¯æ ˆ

- Python 3.10+
- PyTorch
- uv (åŒ…ç®¡ç†)

## é¡¹ç›®ç»“æ„

```
myllm/
â”œâ”€â”€ model.py           # æ¨¡å‹å®šä¹‰
â”œâ”€â”€ train.py           # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ generate.py        # æ–‡æœ¬ç”Ÿæˆ
â”œâ”€â”€ data/input.txt     # è®­ç»ƒæ•°æ®ï¼ˆéœ€ä¸‹è½½ï¼Œä¸åœ¨ git ä¸­ï¼‰
â”œâ”€â”€ articles/          # é…å¥—æ•™ç¨‹æ–‡ç« ï¼ˆä¸­æ–‡ï¼‰
â””â”€â”€ docs/              # é¡¹ç›®æ–‡æ¡£
```

## é…å¥—æ–‡ç« 

æœ¬é¡¹ç›®çš„æ¯ä¸€æ­¥éƒ½æœ‰é…å¥—æ–‡ç« ï¼Œå‘å¸ƒåœ¨å¾®ä¿¡å…¬ä¼—å·ä¸Šï¼š

1. [ä»é›¶æ‰‹æ“å¤§è¯­è¨€æ¨¡å‹ï¼ˆä¸€ï¼‰ï¼šç”¨ä¸‰å›½æ¼”ä¹‰è®­ç»ƒä½ çš„ç¬¬ä¸€ä¸ªè¯­è¨€æ¨¡å‹](articles/01-baby-step-bigram-model.md)
